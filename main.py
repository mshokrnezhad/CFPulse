from utils import *
from urls import URLS
import os
from agents import agent
from pydantic_ai.providers.openrouter import OpenRouterProvider
from pydantic_ai import Agent
from pydantic_ai.models.openai import OpenAIModel
from dotenv import load_dotenv
import asyncio
import logging

# Folder to store downloaded files
DEST_FOLDER = "downloads"

# Load environment variables from .env file
load_dotenv()

# Read configuration from environment variables
ROUTE = os.getenv("ROUTE")
API_KEY = os.getenv("API_KEY")
BASE_URL = os.getenv("BASE_URL")
PAGE_ID = os.getenv("NOTION_PAGE_ID")
NOTION_TOKEN = os.getenv("NOTION_TOKEN")
TMP_FOLDER = os.getenv("TMP_FOLDER")
KB_FILENAME = os.getenv("KB_FILENAME")
RESULTS_FILENAME = os.getenv("RESULTS_FILENAME")
KB_FILE_PATH = TMP_FOLDER + "/" + KB_FILENAME + ".txt"
RESULTS_FILE_PATH = TMP_FOLDER + "/" + RESULTS_FILENAME + ".json"
EMAIL_RECEIVER = os.getenv("EMAIL_RECEIVER")

# Initialize AI model and agent
model = OpenAIModel(ROUTE, provider=OpenRouterProvider(api_key=API_KEY))
agent = Agent(model)


def process_url(entry):
    """
    Process a single URL entry: fetch, compare, print results, and fetch linked <a> hrefs.
    Args:
        entry (dict): Contains 'name', 'base', 'url', and 'element'.
    """
    name = entry['name']
    url = entry['url']
    base = entry['base']
    element = entry.get('element')
    subfolder = os.path.join(DEST_FOLDER, name)
    filename = get_filename_from_url(url)
    # Download the file and get its content and path
    new_content, file_path = download_file(url, subfolder, filename)
    if os.path.exists(file_path):
        # If file exists, compare with old content and extract new links
        with open(file_path, 'r', encoding='utf-8') as f:
            old_content = f.read()
        logging.info(f"--- Checking: {name} ---")
        results = show_diff_and_extract_links(old_content, new_content, base, element)
        for entry in results:
            href = entry['href']
            text = entry.get('text')
            if href:
                fetch_and_store_linked_file(href, TMP_FOLDER, base, name=name, text=text)
    else:
        # First time saving this file
        logging.info(f"--- First time saving: {name} ---")
    # Save the new content
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(new_content)


async def main():
    """
    Main workflow for fetching, processing, analyzing, and emailing CFPs.
    Steps:
    0. Download and check for new CFPs
    1. Load Notion KB
    2. Load and parse CFP files
    3. Analyze CFPs with AI agent
    4. Save results
    5. Email results
    6. Cleanup temporary files
    """
    logging.info("-" * 50)
    logging.info("STEP 0: Finding New CFPs")
    logging.info("-" * 50)

    # Download and process each URL
    for entry in URLS:
        process_url(entry)

    logging.info("-" * 50)
    logging.info("STEP 1: Loading KB")
    logging.info("-" * 50)

    # Download and save Notion KB as Markdown
    if not PAGE_ID or not NOTION_TOKEN:
        logging.error("Please set NOTION_PAGE_ID and NOTION_TOKEN environment variables.")
    else:
        save_notion_markdown(PAGE_ID, NOTION_TOKEN, KB_FILE_PATH)

    logging.info("-" * 50)
    logging.info("STEP 2: Loading CFPs")
    logging.info("-" * 50)

    # Load all processed CFP files
    cfps = load_diff_files(TMP_FOLDER, KB_FILENAME)
    logging.info(f"Total files processed: {len(cfps)}")

    logging.info("-" * 50)
    logging.info("STEP 3: Processing CFPs with AI agent")
    logging.info("-" * 50)

    # Analyze each CFP (except KB) with the AI agent
    for entry in cfps:
        if entry['venue'] != 'KB':  # Skip the KB entry itself
            kb_entry = next((item for item in cfps if item['venue'] == 'KB'), None)
            if kb_entry:
                # Generate prompt and run AI agent
                prompt = generate_cfp_prompt(kb_entry['text'], entry['text'])

                logging.info(f"--- Processing CFP: {entry['title']} ---")
                logging.info(f"Venue: {entry['venue']}")
                logging.info(f"Link: {entry['link']}")
                logging.info("Prompt generated successfully.")
                entry['prompt'] = prompt

                response = await agent.run(prompt)
                entry['response'] = response.output
                logging.info("Response generated by AI agent.")

    # Save all CFP analysis results to JSON
    save_cfps_to_json(cfps, RESULTS_FILE_PATH)

    logging.info("-" * 50)
    logging.info("STEP 5: Emailing results")
    logging.info("-" * 50)

    # Email each CFP result (except KB)
    for entry in cfps:
        if entry['venue'] != 'KB':  # Skip the KB entry
            email_body = create_email_body_for_entry(entry)

            send_email_with_attachment(
                subject=f"CFP Analysis: {entry['title']}",
                body=email_body,
                to_email=EMAIL_RECEIVER,
            )
            logging.info(f"Email sent for: {entry['title']}")

    if any(entry['venue'] != 'KB' for entry in cfps):
        logging.info("All emails sent.")
    else:
        logging.info("No emails to send.")

    logging.info("-" * 50)
    logging.info("STEP 6: Cleaning up temporary files")
    logging.info("-" * 50)

    # Cleanup temporary files (guaranteed to run if called from finally)
    cleanup_tmp_folder(TMP_FOLDER)


if __name__ == "__main__":
    # Log the start of a new run
    logging.info("-" * 50)
    logging.info("-------------------- NEW  RUN --------------------")
    logging.info("-" * 50)
    try:
        # Run the main async workflow
        asyncio.run(main())
    except Exception as e:
        # Log any unhandled exception and send an alert email
        logging.exception("Unhandled exception in main run")
        send_failure_alert(
            subject="CFPulse: Run Failed",
            message=f"An error occurred:\n{str(e)}",
            to_email=EMAIL_RECEIVER
        )
    # Resource cleanup is handled in main()'s finally/cleanup section
